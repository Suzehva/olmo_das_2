{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42f0f568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/Suzehva/time_in_language_models_current/blob/main/ii_accuracy/ii_accuracy.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9aa346e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(0)\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae920eb",
   "metadata": {},
   "source": [
    "## Create all prompts where interchanging should cause the model to switch tenses (past/present/future)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "701f1b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_id = \"EleutherAI/pythia-1.4b-deduped-v0\"\n",
    "revision = None\n",
    "\n",
    "USER = 'aditijb'\n",
    "\n",
    "DATA_DIR = f'/nlp/scr/{USER}/data'\n",
    "MODEL_DIR = f'/nlp/scr/{USER}/models'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e471d3d1",
   "metadata": {},
   "source": [
    "## Heatmap with one prompt\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e39ef940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nnsight is not detected. Please install via 'pip install nnsight' for nnsight backend.\n",
      "loaded model\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyvene\n",
    "from pyvene import embed_to_distrib, top_vals, format_token\n",
    "from pyvene import RepresentationConfig, IntervenableConfig, IntervenableModel\n",
    "from pyvene import VanillaIntervention\n",
    "\n",
    "%config InlineBackend.figure_formats = ['svg']\n",
    "from plotnine import (\n",
    "    ggplot,\n",
    "    geom_tile,\n",
    "    aes,\n",
    "    facet_wrap,\n",
    "    theme,\n",
    "    element_text,\n",
    "    geom_bar,\n",
    "    geom_hline,\n",
    "    scale_y_log10,\n",
    ")\n",
    "\n",
    " \n",
    "config, tokenizer, pythia = pyvene.create_gpt_neox(name=\"EleutherAI/pythia-1.4b-deduped-v0\")\n",
    "# TODO FIGURE THIS OUT!! asked jing if this was correct... I'll see..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b571505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pythia.config.num_hidden_layers 24\n"
     ]
    }
   ],
   "source": [
    "print(\"pythia.config.num_hidden_layers\", pythia.config.num_hidden_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9a6ba1",
   "metadata": {},
   "source": [
    "### Check how years are tokenized!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955bd7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Define years and batch size\n",
    "years = list(range(1700, 2200))\n",
    "batch_size = 64  # adjust depending on GPU memory\n",
    "\n",
    "# Make sure the tokenizer has a pad token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Precompute year token lengths\n",
    "year_token_lengths = {year: len(tokenizer(str(year)).input_ids) for year in years}\n",
    "\n",
    "# Lists to store results\n",
    "past_1token_lt2020 = []\n",
    "future_1token_gt2020 = []\n",
    "past_2token_lt2020 = []\n",
    "future_2token_gt2020 = []\n",
    "\n",
    "# Process in batches\n",
    "for i in range(0, len(years), batch_size):\n",
    "    batch_years = years[i:i+batch_size]\n",
    "    prompts = [f\"In {year} there\" for year in batch_years]\n",
    "    \n",
    "    # Tokenize batch with padding\n",
    "    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True)\n",
    "    \n",
    "    # Run model once for the batch\n",
    "    with torch.no_grad():\n",
    "        outputs = pythia(**inputs)\n",
    "        logits = outputs.logits  # [batch, seq_len, vocab_size]\n",
    "        last_logits = logits[:, -1, :]\n",
    "        probs = torch.softmax(last_logits, dim=-1)\n",
    "        next_token_ids = torch.argmax(probs, dim=-1)\n",
    "    \n",
    "    # Decode and categorize\n",
    "    for year, next_id in zip(batch_years, next_token_ids):\n",
    "        next_token = tokenizer.decode([next_id.item()]).strip().lower()\n",
    "        n_tokens = year_token_lengths[year]\n",
    "\n",
    "        if next_token in [\"was\", \"were\"]:\n",
    "            if n_tokens == 1 and year < 2020:\n",
    "                past_1token_lt2020.append(year)\n",
    "            elif n_tokens == 2 and year < 2020:\n",
    "                past_2token_lt2020.append(year)\n",
    "        elif next_token == \"will\":\n",
    "            if n_tokens == 1 and year > 2020:\n",
    "                future_1token_gt2020.append(year)\n",
    "            elif n_tokens == 2 and year > 2020:\n",
    "                future_2token_gt2020.append(year)\n",
    "\n",
    "# Print results\n",
    "print(\"Past (<2020) predicting 1 token:\", past_1token_lt2020)\n",
    "print(\"Future (>2020) predicting 1 token:\", future_1token_gt2020)\n",
    "print(\"Past (<2020) predicting 2 tokens:\", past_2token_lt2020)\n",
    "print(\"Future (>2020) predicting 2 tokens:\", future_2token_gt2020)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb83e322",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Define the same years\n",
    "years = list(range(1700, 2200))\n",
    "batch_size = 64\n",
    "\n",
    "# Make sure the tokenizer has a pad token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"{'Year':<6} {'Token IDs':<20} {'Decoded Tokens':<20} {'#Tokens':<8} {'Next Token':<10}\")\n",
    "\n",
    "for i in range(0, len(years), batch_size):\n",
    "    batch_years = years[i:i+batch_size]\n",
    "    prompts = [f\"In {year} there\" for year in batch_years]\n",
    "    \n",
    "    # Tokenize batch\n",
    "    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True)\n",
    "    \n",
    "    # Run model once for the batch\n",
    "    with torch.no_grad():\n",
    "        outputs = pythia(**inputs)\n",
    "        logits = outputs.logits\n",
    "        last_logits = logits[:, -1, :]\n",
    "        next_token_ids = torch.argmax(torch.softmax(last_logits, dim=-1), dim=-1)\n",
    "    \n",
    "    # Print year info\n",
    "    for year, next_id in zip(batch_years, next_token_ids):\n",
    "        year_ids = tokenizer(str(year)).input_ids\n",
    "        decoded_year_tokens = [tokenizer.decode([tid]).strip() for tid in year_ids]\n",
    "        n_tokens = len(year_ids)\n",
    "        next_token = tokenizer.decode([next_id.item()]).strip()\n",
    "        print(f\"{year:<6} {year_ids!s:<20} {decoded_year_tokens!s:<20} {n_tokens:<8} {next_token:<10}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3e1caa",
   "metadata": {},
   "source": [
    "### Check how years are tokenized!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2596104a",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PROMPT = \"In 1931 there\"\n",
    "SOURCE_PROMPT = \"In 2023 there\"\n",
    "\n",
    "base = tokenizer(\"In 1931 there\", return_tensors=\"pt\")\n",
    "# base = tokenizer(\"In 1981 there\", return_tensors=\"pt\")\n",
    "sources = [tokenizer(\"In 2023 there\", return_tensors=\"pt\")]\n",
    "tokens = tokenizer.encode(\" was will\")\n",
    "data = []\n",
    "\n",
    "def simple_position_config(model_type, component, layer):\n",
    "    config = IntervenableConfig(\n",
    "        model_type=model_type,\n",
    "        representations=[\n",
    "            RepresentationConfig(\n",
    "                layer,              # layer\n",
    "                component,          # component\n",
    "                \"pos\",              # intervention unit\n",
    "                1,                  # max number of unit\n",
    "            ),\n",
    "        ],\n",
    "        intervention_types=VanillaIntervention,\n",
    "    )\n",
    "    return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a513ad6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "source = sources[0]\n",
    "for sentence in [base, source]:\n",
    "\n",
    "    # Get model logits\n",
    "    with torch.no_grad():\n",
    "        outputs = pythia(**sentence)\n",
    "        logits = outputs.logits  # [batch, seq_len, vocab_size]\n",
    "\n",
    "    # Take the last token's logits (next-token prediction)\n",
    "    last_logits = logits[0, -1]\n",
    "    probs = F.softmax(last_logits, dim=-1)\n",
    "\n",
    "    next_token_id = torch.argmax(probs).item()\n",
    "    next_token_prob = probs[next_token_id].item()\n",
    "\n",
    "    # Decode to a word\n",
    "    next_token = tokenizer.decode([next_token_id])\n",
    "    print(\"sentence:\", tokenizer.decode(sentence.input_ids[0]))\n",
    "    print(f\"Next token: {next_token} (prob={next_token_prob:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98155d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do intervention gathering of hidden layers :)\n",
    "\n",
    "for layer_i in range(pythia.config.num_hidden_layers):\n",
    "    config = simple_position_config(type(pythia), \"block_output\", layer_i) # TODO don't use MLP\n",
    "    intervenable = IntervenableModel(config, pythia)\n",
    "    for pos_i in range(len(base.input_ids[0])):\n",
    "        \n",
    "        _, counterfactual_outputs = intervenable(\n",
    "            base, sources, {\"sources->base\": pos_i}\n",
    "        )\n",
    "        logits = counterfactual_outputs.logits\n",
    "        distrib = torch.softmax(logits, dim=-1)\n",
    "        intervention_token_id = base.input_ids[0][pos_i].item()\n",
    "        intervention_token = format_token(tokenizer, intervention_token_id)\n",
    "        \n",
    "\n",
    "        for token in tokens:\n",
    "            data.append(\n",
    "                {\n",
    "                    \"token\": format_token(tokenizer, token),\n",
    "                    \"prob\": float(distrib[0][-1][token]),\n",
    "                    \"layer\": f\"f{layer_i}\",\n",
    "                    \"pos\": pos_i,\n",
    "                    \"intervention_token\": intervention_token,  # Added this line\n",
    "                    \"type\": \"block_output\",\n",
    "                }\n",
    "            )\n",
    "\n",
    "df = pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8fc0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aditi wrote\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from plotnine import ggplot, aes, geom_tile, facet_wrap, theme, element_text, labs, options\n",
    "\n",
    "# Base and source prompts\n",
    "base_prompt = BASE_PROMPT\n",
    "source_prompt = SOURCE_PROMPT\n",
    "\n",
    "# Tokenize both\n",
    "base_ids = base.input_ids[0]\n",
    "source_ids = sources[0].input_ids[0]\n",
    "\n",
    "base_tokens = [format_token(tokenizer, tid.item()) for tid in base_ids]\n",
    "source_tokens = [format_token(tokenizer, tid.item()) for tid in source_ids]\n",
    "\n",
    "print(\"Base tokens:\", base_tokens)\n",
    "print(\"Source tokens:\", source_tokens)\n",
    "\n",
    "# Make x-axis labels: normally base token, but show swapped token for differences\n",
    "x_labels = []\n",
    "for b_tok, s_tok in zip(base_tokens, source_tokens):\n",
    "    if b_tok != s_tok:\n",
    "        x_labels.append(f\"{b_tok} <- {s_tok}\")\n",
    "    else:\n",
    "        x_labels.append(b_tok)\n",
    "\n",
    "# Map df intervention tokens to these labels\n",
    "df[\"intervention_token_label\"] = pd.Categorical(\n",
    "    [x_labels[pos] for pos in df[\"pos\"]],\n",
    "    categories=x_labels,\n",
    "    ordered=True\n",
    ")\n",
    "\n",
    "# Layers categorical (0 at bottom, max at top)\n",
    "df[\"layer\"] = pd.Categorical(df[\"layer\"], categories=[f\"f{l}\" for l in range(pythia.config.num_hidden_layers)], ordered=True)\n",
    "\n",
    "# Token categorical\n",
    "df[\"token\"] = df[\"token\"].astype(\"category\")\n",
    "\n",
    "# Plot\n",
    "g = (\n",
    "    ggplot(df)\n",
    "    + geom_tile(aes(x=\"intervention_token_label\", y=\"layer\", fill=\"prob\"), raster=False)\n",
    "    + facet_wrap(\"~token\")\n",
    "    + theme(axis_text_x=element_text(rotation=90, ha='right'))\n",
    "    + labs(x=\"Intervention Token (base ← source)\", y=\"Layer\", fill=\"Probability\")\n",
    ")\n",
    "\n",
    "# Optional figure size\n",
    "options.figure_size = (12, 6)\n",
    "\n",
    "g\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff165a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from plotnine import options\n",
    "\n",
    "df[\"layer\"] = df[\"layer\"].astype(\"category\")\n",
    "df[\"token\"] = df[\"token\"].astype(\"category\")\n",
    "\n",
    "# layers with 0 at bottom, 15 at top\n",
    "layer_nodes = [f\"f{l}\" for l in range(pythia.config.num_hidden_layers)]\n",
    "df[\"layer\"] = pd.Categorical(df[\"layer\"], categories=layer_nodes, ordered=True)\n",
    "\n",
    "# Order intervention tokens by their position in the sentence\n",
    "intervention_token_order = []\n",
    "for pos_i in range(len(base.input_ids[0])):\n",
    "    intervention_token_id = base.input_ids[0][pos_i].item()\n",
    "    intervention_token = format_token(tokenizer, intervention_token_id)\n",
    "    intervention_token_order.append(intervention_token)\n",
    "\n",
    "df[\"intervention_token\"] = pd.Categorical(\n",
    "    df[\"intervention_token\"],\n",
    "    categories=intervention_token_order,\n",
    "    ordered=True\n",
    ")\n",
    "\n",
    "g = (\n",
    "    ggplot(df)\n",
    "    + geom_tile(aes(x=\"intervention_token\", y=\"layer\", fill=\"prob\"), raster=False)\n",
    "    + facet_wrap(\"~token\")\n",
    "    + theme(axis_text_x=element_text(rotation=90))\n",
    ")\n",
    "\n",
    "options.figure_size = (8, 6)  # optional\n",
    "g\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "time-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
